name: Run scrapers

on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: '0 9 * * *'

jobs:
  scheduled:
    runs-on: ubuntu-latest
    environment: air-quality-scraper
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.8
    - uses: actions/cache@v2
      name: Configure pip caching
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install Python dependencies
      run: |
        pip install -r requirements.txt
    - name: Run scrapers
      env:
        CONNSTR: ${{ secrets.CONNSTR }}
        REPO_NAME: ${{ secrets.REPO_NAME }}
        SCRAPED_TABLE_NAME: ${{ secrets.SCRAPED_TABLE_NAME }}
        SCRAPER_URL: ${{ secrets.SCRAPER_URL }}
        PROCESS_SCRAPED_DATA_FLAG: ${{ secrets.PROCESS_SCRAPED_DATA_FLAG }}
      run: python 2_scraped_air_quality/run.py
